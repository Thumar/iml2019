{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": "# Tutorial: Generative Adversarial Networks - Advanced Techniques\nThis tutorial is about Generative Models and **Generative Adversarial Networks** (**GANs**).\nIn this tutorial we will implement different types of GANs, which were proposed recently:\n- Vanilla GAN - https://arxiv.org/abs/1406.2661\n- Conditional GAN - https://arxiv.org/abs/1610.09585\n- Wasserstein GAN (WGAN-GP) - https://arxiv.org/abs/1704.00028\n- Spectral Normalization SNGAN - https://arxiv.org/abs/1802.05957\n\nand learn about further techniques to stabilize the training of GANs. (DCGANs, conditioning of the generator ...)\nWe will have a look on three data sets (1 from computer vision, 2 physics data sets)\n- CIFAR10, learn more: https://www.cs.toronto.edu/~kriz/cifar.html\n- Footprints of Air Showers, learn more: https://git.rwth-aachen.de/DavidWalz/airshower\n- Calorimeter Images, learn more: https://doi.org/10.1007/s41781-018-0019-7\n\nAs framework, we make use of TensorFlow:\n- TensorFlow Keras (API shipped with TensorFlow) [learn more \u003e\u003e](https://keras.io/)\n- TensorFlow-GAN (lightweight library for training GANs) [learn more \u003e\u003e](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/gan)\n\n## Basics\n### Generative models\nBefore we jump in to the practical implementation of GANs, we need to introduce _Generative Models_.\nLet us assume we have a bunch of images which forms the distribution of real images $P_{r}$.\nIn our case the distribution consists of several classes horse, airplane, frog, cars etc.\nInstead of training a classifier to be able to label our data we no would like to generate samples which are really\nsimilar to samples out of $P_{r}$.\n \n![CIFAR 10 Image](images/CIFAR10_collection.png)\nSo in a mathematical way we would like to approximate the real distribution $P_{r}$ with a model $P_{\\theta}$.\nWith this *generative* model, we then would like to generate new samples out of our approximation $x \\sim P_{\\theta}$.\n\n### Generative Adversarial Networks\nThe basic idea of generative adversarial networks is to train a **generator network** to learn the underlying distribution.\u003csup\u003e[1](#myfootnote1)\u003c/sup\u003e\nIn other words, we would like to design a generator machine what we can feed with noise and which outputs us nice samples following\nthe distribution of real images $P_{r}$, but which are not part of the training dataset.\nSo in our case, we would like to generate new samples of airplanes, cars, dogs etc..\n \n![Generator Machine](images/generator_machine.png)\n\nThe generator network $G(z)$ gets as input a noise vector $z$ sampled from a muldimensional noise distribution $z \\sim p(z)$.\nThi space of $z$ is often called the latent space. The generator should then map the noise vector $z$ into the data space (the space where our\nreal data samples lie) $\\tilde{x} \\sim G(z)$.\n\nFor the training of the generator network we need feedback, if the generated samples are of good or bad quality.\nBecause a classical supervised loss is incapable for giving a good feedback to the generator network, it is trained in an unsupervised manner.\nSo instead of using \"mean squared error\" or similar metrics, the performance measure is given by a **second** _adversarial_ neural network, which is called disicriminator.\nThis is the fascinating idea of _adversarial training_.\n\n#### Adversarial training\n\nOur adversarial framework consists out of 2 networks:\n- the generator network $G\" (learn the mapping from nois to images)\n- the discriminator \"D\" network (measures the image quality, by discriminating if the images if true or fake)\n\n\nIn a figurative sense the what is the fascinating idea of GANs.\n\n \n$ \\mathcal{L} \u003d \\mathbb{E}_{\\mathbf{x} \\sim p_{data}(\\mathbf{x})} [log(D_w(\\mathbf{x}))] +  \\mathbb{E}_{\\mathbf{z} \\sim p_z(\\mathbf{z})} [log(1-D_w(G(\\mathbf{z})))]$\n\n\n\u003ca name\u003d\"myfootnote1\"\u003e1\u003c/a\u003e: In contrast to e.g. _Variational Autoencoders_ the idea is not to \"directly fit the distribution but to train a generator which approximates the real disitribution directly.\nRemember that in VAEs we learn a mapping in to the latent space where we can \"fit\" a gaussian. Therefore, after the training we can just\ngenerate new samples by sampling from the latentspace using the Gaussian prior.\nBecause most problems are to complex, the Gaussian is not able to capture all modes, this leads to blurry images which is\na well known problem for VAE generated samples.\n"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "pycharm": {
          "is_executing": false,
          "metadata": false,
          "name": "#%%\n"
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "text": [
            "/home/jonas/.local/lib/python2.7/site-packages/h5py/__init__.py:36: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n  from ._conv import register_converters as _register_converters\n/home/jonas/.local/lib/python2.7/site-packages/h5py/__init__.py:45: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n  from . import h5a, h5d, h5ds, h5f, h5fd, h5g, h5r, h5s, h5t, h5p, h5z\n/home/jonas/.local/lib/python2.7/site-packages/h5py/_hl/group.py:22: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n  from .. import h5g, h5i, h5o, h5r, h5t, h5l, h5p\n/usr/local/lib/python2.7/dist-packages/scipy/sparse/lil.py:16: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n  from . import _csparsetools\n",
            "/usr/local/lib/python2.7/dist-packages/scipy/linalg/basic.py:17: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n  from ._solve_toeplitz import levinson\n/usr/local/lib/python2.7/dist-packages/scipy/linalg/__init__.py:202: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n  from ._decomp_update import *\n/usr/local/lib/python2.7/dist-packages/scipy/special/__init__.py:640: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n  from ._ufuncs import *\n/usr/local/lib/python2.7/dist-packages/scipy/special/_ellip_harm.py:7: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n  from ._ellip_harm_2 import _ellipsoid, _ellipsoid_norm\n/usr/local/lib/python2.7/dist-packages/scipy/optimize/_trlib/__init__.py:1: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n  from ._trlib import TRLIBQuadraticSubproblem\n/usr/local/lib/python2.7/dist-packages/scipy/optimize/_numdiff.py:8: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n  from ._group_columns import group_dense, group_sparse\n/usr/local/lib/python2.7/dist-packages/scipy/interpolate/_bsplines.py:9: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n  from . import _bspl\n/usr/local/lib/python2.7/dist-packages/scipy/spatial/__init__.py:94: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n  from .ckdtree import *\n/usr/local/lib/python2.7/dist-packages/scipy/spatial/__init__.py:95: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n  from .qhull import *\n/usr/local/lib/python2.7/dist-packages/scipy/spatial/_spherical_voronoi.py:18: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n  from . import _voronoi\n/usr/local/lib/python2.7/dist-packages/scipy/spatial/distance.py:121: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n  from . import _hausdorff\n/usr/local/lib/python2.7/dist-packages/scipy/ndimage/measurements.py:36: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n  from . import _ni_label\n",
            "/usr/local/lib/python2.7/dist-packages/pandas/_libs/__init__.py:3: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n  from .tslib import iNaT, NaT, Timestamp, Timedelta, OutOfBoundsDatetime\n/usr/local/lib/python2.7/dist-packages/pandas/_libs/__init__.py:3: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192, got 176\n  from .tslib import iNaT, NaT, Timestamp, Timedelta, OutOfBoundsDatetime\n/usr/local/lib/python2.7/dist-packages/pandas/__init__.py:26: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n  from pandas._libs import (hashtable as _hashtable,\n/usr/local/lib/python2.7/dist-packages/pandas/__init__.py:26: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192, got 176\n  from pandas._libs import (hashtable as _hashtable,\n/usr/local/lib/python2.7/dist-packages/pandas/core/dtypes/common.py:6: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n  from pandas._libs import algos, lib\n/usr/local/lib/python2.7/dist-packages/pandas/core/dtypes/common.py:6: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192, got 176\n  from pandas._libs import algos, lib\n/usr/local/lib/python2.7/dist-packages/pandas/core/util/hashing.py:7: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n  from pandas._libs import hashing\n/usr/local/lib/python2.7/dist-packages/pandas/core/util/hashing.py:7: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192, got 176\n  from pandas._libs import hashing\n/usr/local/lib/python2.7/dist-packages/pandas/core/indexes/base.py:6: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n  from pandas._libs import (lib, index as libindex, tslib as libts,\n/usr/local/lib/python2.7/dist-packages/pandas/core/indexes/base.py:6: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192, got 176\n  from pandas._libs import (lib, index as libindex, tslib as libts,\n/usr/local/lib/python2.7/dist-packages/pandas/core/indexes/datetimelike.py:28: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n  from pandas._libs.period import Period\n/usr/local/lib/python2.7/dist-packages/pandas/core/indexes/datetimelike.py:28: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192, got 176\n  from pandas._libs.period import Period\n/usr/local/lib/python2.7/dist-packages/pandas/core/sparse/array.py:32: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n  import pandas._libs.sparse as splib\n/usr/local/lib/python2.7/dist-packages/pandas/core/sparse/array.py:32: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192, got 176\n  import pandas._libs.sparse as splib\n/usr/local/lib/python2.7/dist-packages/pandas/core/window.py:36: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n  import pandas._libs.window as _window\n/usr/local/lib/python2.7/dist-packages/pandas/core/window.py:36: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192, got 176\n  import pandas._libs.window as _window\n",
            "/usr/local/lib/python2.7/dist-packages/pandas/core/groupby.py:66: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n  from pandas._libs import lib, groupby as libgroupby, Timestamp, NaT, iNaT\n/usr/local/lib/python2.7/dist-packages/pandas/core/groupby.py:66: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192, got 176\n  from pandas._libs import lib, groupby as libgroupby, Timestamp, NaT, iNaT\n/usr/local/lib/python2.7/dist-packages/pandas/core/reshape/reshape.py:30: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n  from pandas._libs import algos as _algos, reshape as _reshape\n/usr/local/lib/python2.7/dist-packages/pandas/core/reshape/reshape.py:30: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192, got 176\n  from pandas._libs import algos as _algos, reshape as _reshape\n/usr/local/lib/python2.7/dist-packages/pandas/io/parsers.py:43: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n  import pandas._libs.parsers as parsers\n/usr/local/lib/python2.7/dist-packages/pandas/io/parsers.py:43: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192, got 176\n  import pandas._libs.parsers as parsers\n"
          ],
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": [
            "eos access: ✗\n",
            "(\u0027TensorFLow version\u0027, \u00271.13.1\u0027)\n"
          ],
          "output_type": "stream"
        }
      ],
      "source": "import numpy as np\nimport tensorflow as tf\nfrom plotting import plot_images\nimport tutorial\nlayers \u003d tf.layers\nprint(\"TensorFLow version\", tf.__version__)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% md\n"
        }
      },
      "source": "Let\u0027s start to build a data pipeline.\nFirst we need to define our Data generator.\nThe generator should output real samples (input for the discriminator) and noise (input for the generator)\nThe variable LATENT_DIM defines the dimensionality of the latent space of the generator.\n(The noise distribution we sample from)."
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      },
      "outputs": [],
      "source": "def generator(LATENT_DIM):\n    while True:\n        (x_train, y_train), (x_test, y_test) \u003d tf.keras.datasets.mnist.load_data()\n        images \u003d (np.expand_dims(x_train, axis\u003d-1)) / 255.\n        images \u003d images.astype(np.float32)\n        noise \u003d np.random.randn(60000, LATENT_DIM).reshape(60000, LATENT_DIM)\n        idx \u003d np.random.permutation(60000)\n        noise \u003d noise[idx]\n        images \u003d images[idx]\n        for i in range(60000):\n            yield (noise[i], images[i])"
    },
    {
      "cell_type": "markdown",
      "source": "Let us now check if our generator is working",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "outputs": [
        {
          "data": {
            "text/plain": "(1, 2)"
          },
          "metadata": {},
          "output_type": "execute_result",
          "execution_count": 6
        }
      ],
      "source": "import itertools\ntest_image \u003d np.array(list(itertools.islice(generator(64), 1)))\ntest_image.shape",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "To train our estimator we can make create a TensorflowDataset out of our data generator.\nThe function outputs a batches of our dataset.",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% md\n"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "outputs": [],
      "source": "def batch_dataset(BATCH_SIZE, LATENT_DIM, generator_fn):\n    Dataset \u003d tf.data.Dataset.from_generator(\n        lambda: generator_fn(LATENT_DIM), output_types\u003d(tf.float32, tf.float32),\n        output_shapes\u003d(tf.TensorShape((LATENT_DIM,)), tf.TensorShape((28, 28, 1))))\n    return Dataset.batch(BATCH_SIZE)",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "For training GANs we need to further define our generator and discriminator network.\nWe start by defining our generator network, which should map from our noise space into the space of out images (LATENT_DIM --\u003e IMAGE_DIM)",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% md\n"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "outputs": [],
      "source": "def generator_fn(x, latent_dim\u003dLATENT_DIM):\n    x \u003d layers.Dense(7 * 7 * 128, activation\u003d\u0027relu\u0027, input_shape\u003d(latent_dim,))(x)  #\n    x \u003d tf.reshape(x, shape\u003d[BATCH_SIZE, 7, 7, 128])\n    x \u003d layers.Conv2DTranspose(128, (5, 5), strides\u003d(2, 2), padding\u003d\u0027same\u0027, activation\u003d\u0027relu\u0027)(x)\n    x \u003d layers.Conv2DTranspose(64, (5, 5), strides\u003d(2, 2), padding\u003d\u0027same\u0027, activation\u003d\u0027relu\u0027)(x)\n    x \u003d layers.Conv2D(1, (5, 5), padding\u003d\u0027same\u0027, activation\u003d\u0027sigmoid\u0027)(x)\n    return x",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "After defining our generator network we need now to implement our discriminator.\nThe task of the discriminator is to measure the similarity between the fake images (output of the generator) and the real images.\nSo, the network maps from the image space into a 1D space where we can measure the \u0027distance\u0027 between the distributions of the real and generated images.  (IMAGE_DIM --\u003e 1)",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "outputs": [],
      "source": "def discriminator_fn(x, drop_rate\u003d0.25):\n    \"\"\" Discriminator network \"\"\"\n    x \u003d layers.Conv2D(32, (5, 5), padding\u003d\u0027same\u0027, strides\u003d(2, 2), activation\u003d\u0027relu\u0027, input_shape\u003d(28, 28, 1))(x)\n    x \u003d tf.nn.leaky_relu(x, 0.2)\n    x \u003d layers.Conv2D(64, (5, 5), padding\u003d\u0027same\u0027, strides\u003d(2, 2), activation\u003d\u0027relu\u0027)(x)\n    x \u003d tf.nn.leaky_relu(x, 0.2)\n    x \u003d layers.Conv2D(128, (5, 5), padding\u003d\u0027same\u0027, strides\u003d(2, 2), activation\u003d\u0027relu\u0027)(x)\n    x \u003d tf.nn.leaky_relu(x, 0.2)\n    x \u003d layers.Flatten()(x)\n    x \u003d layers.Dense(256)(x)\n    x \u003d tf.nn.leaky_relu(x, 0.2)\n    x \u003d layers.Dense(1)(x)\n    return x",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "As Wasserstein-1 is a meaningful distance measure for disjoint distributions let\u0027s use it as objective of our GAN training.\nWe can very easily make use of the losses predefined in tf.contrib.gan.\nThe are 2 possible constraints to construct the Wasserstein distance:\n- Use weight clipping\n- Penalize the gradient\n\n(Easy interpretation: We need a constraint to train the discriminator to convergence, otherwise the discriminator could focus on one feature which differs between real and fake samples and won\u0027t converge)\nWeight clamping will heavily reduce the capacity of the discriminator which is unfavourable.\nSo let use use the Gradient Penalty (https://arxiv.org/abs/1704.00028):\nBy penalizing the gradient to be smaller than 1, we enforce the lipschitz constraint needed to construct Wasserstein using the Kantorovich-Rubinstein duality(https://cedricvillani.org/wp-content/uploads/2012/08/preprint-1.pdf)",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% md\n"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "outputs": [],
      "source": "def discrimintator_loss(model, add_summaries\u003dTrue):\n\n    loss \u003d tf.contrib.gan.losses.wasserstein_discriminator_loss(model, add_summaries\u003dadd_summaries)\n    gp_loss \u003d GP * tf.contrib.gan.losses.wasserstein_gradient_penalty(model, epsilon\u003d1e-10, one_sided\u003dTrue, add_summaries\u003dadd_summaries)\n    loss +\u003d gp_loss\n\n    if add_summaries:\n        tf.summary.scalar(\u0027discriminator_loss\u0027, loss)\n\n    return loss",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "After defining our loss we can choose our training parameters",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% md\n"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "outputs": [],
      "source": "BATCH_SIZE \u003d 32  # number of samples fed into the framework in each iteration\nLATENT_DIM \u003d 64  # dimension of the generators latent space\nGEN_LR \u003d 0.001   # learning rate of the generator\nDIS_LR \u003d 0.0001  # learning rate of the discriminator\nITER \u003d 1000      # framework iterations\nLOG_DIR \u003d \".\"    # directory of the estimator (to save the graph and checkpoints)\ndir \u003d tutorial.make_dir(LOG_DIR, \"WGAN_GP\")\nGP \u003d 10          # factor to scale the gradient penalty (higher means larger enforcing the Lipschitz constrain)\nN_CRIT \u003d 5       # number of critic iterations per generator iterations.",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "Now we can very easily implement our framework as estimator using tfgan.\nThis will heavily simplify our training procedure.",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% md\n"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "outputs": [
        {
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Using config: {\u0027_save_checkpoints_secs\u0027: None, \u0027_num_ps_replicas\u0027: 0, \u0027_keep_checkpoint_max\u0027: 1, \u0027_task_type\u0027: \u0027worker\u0027, \u0027_global_id_in_cluster\u0027: 0, \u0027_is_chief\u0027: True, \u0027_cluster_spec\u0027: \u003ctensorflow.python.training.server_lib.ClusterSpec object at 0x7feb89959890\u003e, \u0027_model_dir\u0027: \u0027./WGAN_GP_train_2019-04-04_17:03:06\u0027, \u0027_protocol\u0027: None, \u0027_save_checkpoints_steps\u0027: 200, \u0027_keep_checkpoint_every_n_hours\u0027: 10000, \u0027_service\u0027: None, \u0027_session_config\u0027: allow_soft_placement: true\ngraph_options {\n  rewrite_options {\n    meta_optimizer_iterations: ONE\n  }\n}\n, \u0027_tf_random_seed\u0027: None, \u0027_save_summary_steps\u0027: 10, \u0027_device_fn\u0027: None, \u0027_experimental_distribute\u0027: None, \u0027_num_worker_replicas\u0027: 1, \u0027_task_id\u0027: 0, \u0027_log_step_count_steps\u0027: 100, \u0027_evaluation_master\u0027: \u0027\u0027, \u0027_eval_distribute\u0027: None, \u0027_train_distribute\u0027: None, \u0027_master\u0027: \u0027\u0027}\n"
          ],
          "output_type": "stream"
        }
      ],
      "source": "tfgan \u003d tf.contrib.gan\ngan_estimator \u003d tfgan.estimator.GANEstimator(\n    dir,\n    generator_fn\u003dgenerator_fn,\n    discriminator_fn\u003ddiscriminator_fn,\n    generator_loss_fn\u003dtfgan.losses.wasserstein_generator_loss,\n    discriminator_loss_fn\u003ddiscrimintator_loss,\n    generator_optimizer\u003dtf.train.AdamOptimizer(GEN_LR, 0.5),\n    discriminator_optimizer\u003dtf.train.AdamOptimizer(DIS_LR, 0.5),\n    get_hooks_fn\u003dtfgan.get_sequential_train_hooks(tfgan.GANTrainSteps(1, N_CRIT)),\n    config\u003dtf.estimator.RunConfig(save_summary_steps\u003d10, keep_checkpoint_max\u003d1, save_checkpoints_steps\u003d200),\n    use_loss_summaries\u003dTrue)",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "Let us train our framework using our gan_estimator and our data_pipeline",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "outputs": [
        {
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Calling model_fn.\n",
            "WARNING:tensorflow:From /home/jonas/.local/lib/python2.7/site-packages/tensorflow/contrib/gan/python/losses/python/losses_impl.py:101: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse tf.cast instead.\n",
            "WARNING:tensorflow:From /home/jonas/.local/lib/python2.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse tf.cast instead.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Create CheckpointSaverHook.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Saving checkpoints for 0 into ./WGAN_GP_train_2019-04-04_17:03:06/model.ckpt.\n",
            "INFO:tensorflow:loss \u003d 0.041437, step \u003d 1\n",
            "INFO:tensorflow:global_step/sec: 0.434456\n",
            "INFO:tensorflow:loss \u003d -5.485588, step \u003d 101 (230.182 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 200 into ./WGAN_GP_train_2019-04-04_17:03:06/model.ckpt.\n",
            "WARNING:tensorflow:From /home/jonas/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py:966: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse standard file APIs to delete files with this prefix.\n",
            "INFO:tensorflow:global_step/sec: 0.427456\n",
            "INFO:tensorflow:loss \u003d 1.0505079, step \u003d 201 (233.934 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.388586\n",
            "INFO:tensorflow:loss \u003d 0.48207843, step \u003d 301 (257.343 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 400 into ./WGAN_GP_train_2019-04-04_17:03:06/model.ckpt.\n",
            "INFO:tensorflow:global_step/sec: 0.453063\n",
            "INFO:tensorflow:loss \u003d -1.0133413, step \u003d 401 (220.718 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.560305\n",
            "INFO:tensorflow:loss \u003d -2.5719464, step \u003d 501 (178.475 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 600 into ./WGAN_GP_train_2019-04-04_17:03:06/model.ckpt.\n",
            "INFO:tensorflow:global_step/sec: 0.576378\n",
            "INFO:tensorflow:loss \u003d -0.985567, step \u003d 601 (173.497 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.587816\n",
            "INFO:tensorflow:loss \u003d -5.8786507, step \u003d 701 (170.121 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 800 into ./WGAN_GP_train_2019-04-04_17:03:06/model.ckpt.\n",
            "INFO:tensorflow:global_step/sec: 0.588373\n",
            "INFO:tensorflow:loss \u003d -1.4504924, step \u003d 801 (169.960 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.538998\n",
            "INFO:tensorflow:loss \u003d -3.8765674, step \u003d 901 (185.530 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 1000 into ./WGAN_GP_train_2019-04-04_17:03:06/model.ckpt.\n",
            "INFO:tensorflow:Loss for final step: 0.5054107.\n"
          ],
          "output_type": "stream"
        },
        {
          "data": {
            "text/plain": "\u003ctensorflow.contrib.gan.python.estimator.python.gan_estimator_impl.GANEstimator at 0x7feb89959e10\u003e"
          },
          "metadata": {},
          "output_type": "execute_result",
          "execution_count": 21
        }
      ],
      "source": "# gan_estimator.train(lambda: batch_dataset(BATCH_SIZE, LATENT_DIM, generator), steps\u003dITER)",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% md\n"
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 2",
      "language": "python",
      "name": "python2"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}