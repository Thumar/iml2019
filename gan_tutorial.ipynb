{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": "# Tutorial: Generative Adversarial Networks - Advanced Techniques\nThis tutorial is about Generative Models and **Generative Adversarial Networks** (**GANs**).\nIn this tutorial we will implement different types of GANs, which were proposed recently:\n- Vanilla GAN - https://arxiv.org/abs/1406.2661\n- Conditional GAN - https://arxiv.org/abs/1610.09585\n- Wasserstein GAN (WGAN-GP) - https://arxiv.org/abs/1704.00028\n- Spectral Normalization SNGAN - https://arxiv.org/abs/1802.05957\n\nand learn about further techniques to stabilize the training of GANs. (DCGANs, conditioning of the generator ...)\nWe will have a look on three data sets (1 from computer vision, 2 physics data sets)\n- CIFAR10, learn more: https://www.cs.toronto.edu/~kriz/cifar.html\n- Footprints of Air Showers, learn more: https://git.rwth-aachen.de/DavidWalz/airshower\n- Calorimeter Images, learn more: https://doi.org/10.1007/s41781-018-0019-7\n\nAs framework, we make use of TensorFlow:\n- TensorFlow Keras (API shipped with TensorFlow) [learn more \u003e\u003e](https://keras.io/)\n- TensorFlow-GAN (lightweight library for training GANs) [learn more \u003e\u003e](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/gan)\n\n## Basics\n### Generative models\nBefore we jump in to the practical implementation of GANs, we need to introduce _Generative Models_.\nLet us assume we have a bunch of images which forms the distribution of real images $P_{r}$.\nIn our case the distribution consists of several classes horse, airplane, frog, cars etc.\nInstead of training a classifier to be able to label our data we no would like to generate samples which are really\nsimilar to samples out of $P_{r}$.\n \n![CIFAR 10 Image](images/CIFAR10_collection.png)\nSo in a mathematical way we would like to approximate the real distribution $P_{r}$ with a model $P_{\\theta}$.\nWith this *generative* model, we then would like to generate new samples out of our approximation $x \\sim P_{\\theta}$.\n\n### Generative Adversarial Networks\nThe basic idea of generative adversarial networks is to train a **generator network** to learn the underlying distribution.\u003csup\u003e[1](#myfootnote1)\u003c/sup\u003e\nIn other words, we would like to design a generator machine what we can feed with noise and which outputs us nice samples following\nthe distribution of real images $P_{r}$, but which are not part of the training dataset.\nSo in our case, we would like to generate new samples of airplanes, cars, dogs etc..\n \n![Generator Machine](images/generator_machine.png)\n\nThe generator network $G(z)$ gets as input a noise vector $z$ sampled from a muldimensional noise distribution $z \\sim p(z)$.\nThi space of $z$ is often called the latent space. The generator should then map the noise vector $z$ into the data space (the space where our\nreal data samples lie) $\\tilde{x} \\sim G(z)$.\n\nFor the training of the generator network we need feedback, if the generated samples are of good or bad quality.\nBecause a classical supervised loss is incapable for giving a good feedback to the generator network, it is trained in an unsupervised manner.\nSo instead of using \"mean squared error\" or similar metrics, the performance measure is given by a **second** _adversarial_ neural network, which is called disicriminator.\nIn the vanilla GAN setup, the way of measuring the quality of the generated sampels should be given by a classifier which is trained\nto classify between fake `class\u003d0` (generated by our generator network) and real images `class\u003d1`.\nThe clue is now, that the generator should try to fool the discriminator.\nSo, by adapting the generator weights the discriminator should fail to identify the fake images, and should output `class\u003d1`\nwhen input the generated images.\nIn simple words, the generator should change the weights to generate images which holds features,\nwhich the discriminator identifies as features from real images.\nThis is the fascinating idea of _adversarial training_.\n\n![Figurative Sketch: Adversarial Training](images/adversarial_training_sketch.png)\nIn a very figurative sense, the discriminator could be seen as painter which is able to classify between\nreal images and fake images, because he knows a little bit about colors and images.\nThe idea is, to fool the painter (discriminator) by changing the parameters of our generator machine.\nBecause, the painter has knowledge how \"real images\" look like, using the feedback of him helps to generate\nimages of better quality.\n\n#### Adversarial training\n\nOur adversarial framework consists out of 2 networks:\n- the generator network $G$ (learn the mapping from nois to images)\n- the discriminator $D$ network (measures the image quality, by discriminating if the images are true `class\u003d1` or fake `class\u003d0`)\n\nThe training procedure of the framework is as follows:\n    1. Discriminator update: Train the discriminator to classify between fake and real images\n    2. Generator update: Train the generator to fool the generator\n    -  Repeat from beginning\n\n\n\n##### Discriminator update\nSample noise from the latent space $z \\sim p(z)$\nGenerate new fake samples by feed the noise in to the generator $\\tilde{x} \\sim G(z)$\nSample real samples from the real distribution $ x \\sim P_{r}$\nTrain the discriminator using the binary cross entropy by changing the weights $w$:\n\n$ \\mathcal{L_{Dis}} \u003d \\min{ -\\mathbb{E}_{\\mathbf{x} \\sim p_{data}(\\mathbf{x})} [log(D_w(\\mathbf{x}))] - \\mathbb{E}_{\\mathbf{z} \\sim p_z(\\mathbf{z})} [log(1-D_w(G_{\\theta}(\\mathbf{z})))]}$\n\n##### Generator update\nSample noise from the latent space $z \\sim p(z)$\nGenerate new fake samples by feed the noise in to the generator $\\tilde{x} \\sim G(z)$\nFreeze the weights of the discriminator $w$\nTrain the generator to fool the discriminator, by adapting the weights $\\theta$:\n\n$ \\mathcal{L_{Gen}} \u003d \\max{ -\\mathbb{E}_{\\mathbf{z} \\sim p_z(\\mathbf{z})} [log(1-D_w(G_{\\theta}(\\mathbf{z})))]}$\n\n\n \n\n\n### Architectures\nThe generator maps from the latent space into the data space.\nHence the input should be a 1D-vector of noise variables and the output should have image dimensions.\nIn this case the output will have the dimension `(32, 32, 3)`. (3 color channels: RGB)\n\n[Radford, Metz, and Chintala](https://arxiv.org/abs/1511.06434) proposed stable architectures for generator and discriminator networks.\nImportant DCGAN Guidelines:\n- Replace fully connected layers with convolutional layers\n- Do not use pooling layers, use striding instead\n- Make use of batch normalization in generator and discriminator to stabilize training\u003csup\u003e[2](#myfootnote1)\u003c/sup\u003e\n- Use LeakyRelu activation in discriminator for better feedback\u003csup\u003e[3](#myfootnote1)\u003c/sup\u003e\n- Use a pyramidal topolgy in the generator by using transposed convolutions, to support a simple and strucutred latent space\n\n![Pyramidal topolgy of the genrator as proposed by Radford, Metz, and Chintala](images/DCGAN_generator.png)\n\n\n\u003ca name\u003d\"myfootnote1\"\u003e1\u003c/a\u003e: In contrast to e.g. _Variational Autoencoders_ the idea is not to \"directly fit the distribution but to train a generator which approximates the real disitribution directly.\nRemember that in VAEs we learn a mapping in to the latent space where we can \"fit\" a gaussian. Therefore, after the training we can just\ngenerate new samples by sampling from the latentspace using the Gaussian prior.\nBecause most problems are to complex, the Gaussian is not able to capture all modes, this leads to blurry images which is\na well known problem for VAE generated samples.\n\n\u003ca name\u003d\"myfootnote2\"\u003e2\u003c/a\u003e: Batch normalization is very important here. Due to the normalization of the gradients, it stabilizes the training.\nAs we will see later normalizing the gradients deliverd by the discriminator, is crucial.\n\n\u003ca name\u003d\"myfootnote3\"\u003e3\u003c/a\u003e: Using ReLU in the discriminator woul lead to sparase gradient (no negative gradient could propagate back).\nUsing LeakyReLU provides better feedback.\n \nimport numpy as np\nimport tensorflow as tf\nfrom plotting import plot_images\nimport tutorial\nlayers \u003d tf.layers\nprint(\"TensorFLow version\", tf.__version__)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% md\n"
        }
      },
      "source": "Let\u0027s start to build a data pipeline.\nFirst we need to define our Data generator.\nThe generator should output real samples (input for the discriminator) and noise (input for the generator)\nThe variable LATENT_DIM defines the dimensionality of the latent space of the generator.\n(The noise distribution we sample from)."
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      },
      "outputs": [],
      "source": "def generator(LATENT_DIM):\n    while True:\n        (x_train, y_train), (x_test, y_test) \u003d tf.keras.datasets.mnist.load_data()\n        images \u003d (np.expand_dims(x_train, axis\u003d-1)) / 255.\n        images \u003d images.astype(np.float32)\n        noise \u003d np.random.randn(60000, LATENT_DIM).reshape(60000, LATENT_DIM)\n        idx \u003d np.random.permutation(60000)\n        noise \u003d noise[idx]\n        images \u003d images[idx]\n        for i in range(60000):\n            yield (noise[i], images[i])"
    },
    {
      "cell_type": "markdown",
      "source": "Let us now check if our generator is working",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "outputs": [
        {
          "data": {
            "text/plain": "(1, 2)"
          },
          "metadata": {},
          "output_type": "execute_result",
          "execution_count": 6
        }
      ],
      "source": "import itertools\ntest_image \u003d np.array(list(itertools.islice(generator(64), 1)))\ntest_image.shape",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "To train our estimator we can make create a TensorflowDataset out of our data generator.\nThe function outputs a batches of our dataset.",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% md\n"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "outputs": [],
      "source": "def batch_dataset(BATCH_SIZE, LATENT_DIM, generator_fn):\n    Dataset \u003d tf.data.Dataset.from_generator(\n        lambda: generator_fn(LATENT_DIM), output_types\u003d(tf.float32, tf.float32),\n        output_shapes\u003d(tf.TensorShape((LATENT_DIM,)), tf.TensorShape((28, 28, 1))))\n    return Dataset.batch(BATCH_SIZE)",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "For training GANs we need to further define our generator and discriminator network.\nWe start by defining our generator network, which should map from our noise space into the space of out images (LATENT_DIM --\u003e IMAGE_DIM)",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% md\n"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "outputs": [],
      "source": "def generator_fn(x, latent_dim\u003dLATENT_DIM):\n    x \u003d layers.Dense(7 * 7 * 128, activation\u003d\u0027relu\u0027, input_shape\u003d(latent_dim,))(x)  #\n    x \u003d tf.reshape(x, shape\u003d[BATCH_SIZE, 7, 7, 128])\n    x \u003d layers.Conv2DTranspose(128, (5, 5), strides\u003d(2, 2), padding\u003d\u0027same\u0027, activation\u003d\u0027relu\u0027)(x)\n    x \u003d layers.Conv2DTranspose(64, (5, 5), strides\u003d(2, 2), padding\u003d\u0027same\u0027, activation\u003d\u0027relu\u0027)(x)\n    x \u003d layers.Conv2D(1, (5, 5), padding\u003d\u0027same\u0027, activation\u003d\u0027sigmoid\u0027)(x)\n    return x",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "After defining our generator network we need now to implement our discriminator.\nThe task of the discriminator is to measure the similarity between the fake images (output of the generator) and the real images.\nSo, the network maps from the image space into a 1D space where we can measure the \u0027distance\u0027 between the distributions of the real and generated images.  (IMAGE_DIM --\u003e 1)",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "outputs": [],
      "source": "def discriminator_fn(x, drop_rate\u003d0.25):\n    \"\"\" Discriminator network \"\"\"\n    x \u003d layers.Conv2D(32, (5, 5), padding\u003d\u0027same\u0027, strides\u003d(2, 2), activation\u003d\u0027relu\u0027, input_shape\u003d(28, 28, 1))(x)\n    x \u003d tf.nn.leaky_relu(x, 0.2)\n    x \u003d layers.Conv2D(64, (5, 5), padding\u003d\u0027same\u0027, strides\u003d(2, 2), activation\u003d\u0027relu\u0027)(x)\n    x \u003d tf.nn.leaky_relu(x, 0.2)\n    x \u003d layers.Conv2D(128, (5, 5), padding\u003d\u0027same\u0027, strides\u003d(2, 2), activation\u003d\u0027relu\u0027)(x)\n    x \u003d tf.nn.leaky_relu(x, 0.2)\n    x \u003d layers.Flatten()(x)\n    x \u003d layers.Dense(256)(x)\n    x \u003d tf.nn.leaky_relu(x, 0.2)\n    x \u003d layers.Dense(1)(x)\n    return x",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "As Wasserstein-1 is a meaningful distance measure for disjoint distributions let\u0027s use it as objective of our GAN training.\nWe can very easily make use of the losses predefined in tf.contrib.gan.\nThe are 2 possible constraints to construct the Wasserstein distance:\n- Use weight clipping\n- Penalize the gradient\n\n(Easy interpretation: We need a constraint to train the discriminator to convergence, otherwise the discriminator could focus on one feature which differs between real and fake samples and won\u0027t converge)\nWeight clamping will heavily reduce the capacity of the discriminator which is unfavourable.\nSo let use use the Gradient Penalty (https://arxiv.org/abs/1704.00028):\nBy penalizing the gradient to be smaller than 1, we enforce the lipschitz constraint needed to construct Wasserstein using the Kantorovich-Rubinstein duality(https://cedricvillani.org/wp-content/uploads/2012/08/preprint-1.pdf)",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% md\n"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "outputs": [],
      "source": "def discrimintator_loss(model, add_summaries\u003dTrue):\n\n    loss \u003d tf.contrib.gan.losses.wasserstein_discriminator_loss(model, add_summaries\u003dadd_summaries)\n    gp_loss \u003d GP * tf.contrib.gan.losses.wasserstein_gradient_penalty(model, epsilon\u003d1e-10, one_sided\u003dTrue, add_summaries\u003dadd_summaries)\n    loss +\u003d gp_loss\n\n    if add_summaries:\n        tf.summary.scalar(\u0027discriminator_loss\u0027, loss)\n\n    return loss",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "After defining our loss we can choose our training parameters",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% md\n"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "outputs": [],
      "source": "BATCH_SIZE \u003d 32  # number of samples fed into the framework in each iteration\nLATENT_DIM \u003d 64  # dimension of the generators latent space\nGEN_LR \u003d 0.001   # learning rate of the generator\nDIS_LR \u003d 0.0001  # learning rate of the discriminator\nITER \u003d 1000      # framework iterations\nLOG_DIR \u003d \".\"    # directory of the estimator (to save the graph and checkpoints)\ndir \u003d tutorial.make_dir(LOG_DIR, \"WGAN_GP\")\nGP \u003d 10          # factor to scale the gradient penalty (higher means larger enforcing the Lipschitz constrain)\nN_CRIT \u003d 5       # number of critic iterations per generator iterations.",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "Now we can very easily implement our framework as estimator using tfgan.\nThis will heavily simplify our training procedure.",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% md\n"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "outputs": [
        {
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Using config: {\u0027_save_checkpoints_secs\u0027: None, \u0027_num_ps_replicas\u0027: 0, \u0027_keep_checkpoint_max\u0027: 1, \u0027_task_type\u0027: \u0027worker\u0027, \u0027_global_id_in_cluster\u0027: 0, \u0027_is_chief\u0027: True, \u0027_cluster_spec\u0027: \u003ctensorflow.python.training.server_lib.ClusterSpec object at 0x7feb89959890\u003e, \u0027_model_dir\u0027: \u0027./WGAN_GP_train_2019-04-04_17:03:06\u0027, \u0027_protocol\u0027: None, \u0027_save_checkpoints_steps\u0027: 200, \u0027_keep_checkpoint_every_n_hours\u0027: 10000, \u0027_service\u0027: None, \u0027_session_config\u0027: allow_soft_placement: true\ngraph_options {\n  rewrite_options {\n    meta_optimizer_iterations: ONE\n  }\n}\n, \u0027_tf_random_seed\u0027: None, \u0027_save_summary_steps\u0027: 10, \u0027_device_fn\u0027: None, \u0027_experimental_distribute\u0027: None, \u0027_num_worker_replicas\u0027: 1, \u0027_task_id\u0027: 0, \u0027_log_step_count_steps\u0027: 100, \u0027_evaluation_master\u0027: \u0027\u0027, \u0027_eval_distribute\u0027: None, \u0027_train_distribute\u0027: None, \u0027_master\u0027: \u0027\u0027}\n"
          ],
          "output_type": "stream"
        }
      ],
      "source": "tfgan \u003d tf.contrib.gan\ngan_estimator \u003d tfgan.estimator.GANEstimator(\n    dir,\n    generator_fn\u003dgenerator_fn,\n    discriminator_fn\u003ddiscriminator_fn,\n    generator_loss_fn\u003dtfgan.losses.wasserstein_generator_loss,\n    discriminator_loss_fn\u003ddiscrimintator_loss,\n    generator_optimizer\u003dtf.train.AdamOptimizer(GEN_LR, 0.5),\n    discriminator_optimizer\u003dtf.train.AdamOptimizer(DIS_LR, 0.5),\n    get_hooks_fn\u003dtfgan.get_sequential_train_hooks(tfgan.GANTrainSteps(1, N_CRIT)),\n    config\u003dtf.estimator.RunConfig(save_summary_steps\u003d10, keep_checkpoint_max\u003d1, save_checkpoints_steps\u003d200),\n    use_loss_summaries\u003dTrue)",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "Let us train our framework using our gan_estimator and our data_pipeline",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "outputs": [
        {
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Calling model_fn.\n",
            "WARNING:tensorflow:From /home/jonas/.local/lib/python2.7/site-packages/tensorflow/contrib/gan/python/losses/python/losses_impl.py:101: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse tf.cast instead.\n",
            "WARNING:tensorflow:From /home/jonas/.local/lib/python2.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse tf.cast instead.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Create CheckpointSaverHook.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Saving checkpoints for 0 into ./WGAN_GP_train_2019-04-04_17:03:06/model.ckpt.\n",
            "INFO:tensorflow:loss \u003d 0.041437, step \u003d 1\n",
            "INFO:tensorflow:global_step/sec: 0.434456\n",
            "INFO:tensorflow:loss \u003d -5.485588, step \u003d 101 (230.182 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 200 into ./WGAN_GP_train_2019-04-04_17:03:06/model.ckpt.\n",
            "WARNING:tensorflow:From /home/jonas/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py:966: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse standard file APIs to delete files with this prefix.\n",
            "INFO:tensorflow:global_step/sec: 0.427456\n",
            "INFO:tensorflow:loss \u003d 1.0505079, step \u003d 201 (233.934 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.388586\n",
            "INFO:tensorflow:loss \u003d 0.48207843, step \u003d 301 (257.343 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 400 into ./WGAN_GP_train_2019-04-04_17:03:06/model.ckpt.\n",
            "INFO:tensorflow:global_step/sec: 0.453063\n",
            "INFO:tensorflow:loss \u003d -1.0133413, step \u003d 401 (220.718 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.560305\n",
            "INFO:tensorflow:loss \u003d -2.5719464, step \u003d 501 (178.475 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 600 into ./WGAN_GP_train_2019-04-04_17:03:06/model.ckpt.\n",
            "INFO:tensorflow:global_step/sec: 0.576378\n",
            "INFO:tensorflow:loss \u003d -0.985567, step \u003d 601 (173.497 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.587816\n",
            "INFO:tensorflow:loss \u003d -5.8786507, step \u003d 701 (170.121 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 800 into ./WGAN_GP_train_2019-04-04_17:03:06/model.ckpt.\n",
            "INFO:tensorflow:global_step/sec: 0.588373\n",
            "INFO:tensorflow:loss \u003d -1.4504924, step \u003d 801 (169.960 sec)\n",
            "INFO:tensorflow:global_step/sec: 0.538998\n",
            "INFO:tensorflow:loss \u003d -3.8765674, step \u003d 901 (185.530 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 1000 into ./WGAN_GP_train_2019-04-04_17:03:06/model.ckpt.\n",
            "INFO:tensorflow:Loss for final step: 0.5054107.\n"
          ],
          "output_type": "stream"
        },
        {
          "data": {
            "text/plain": "\u003ctensorflow.contrib.gan.python.estimator.python.gan_estimator_impl.GANEstimator at 0x7feb89959e10\u003e"
          },
          "metadata": {},
          "output_type": "execute_result",
          "execution_count": 21
        }
      ],
      "source": "# gan_estimator.train(lambda: batch_dataset(BATCH_SIZE, LATENT_DIM, generator), steps\u003dITER)",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% md\n"
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 2",
      "language": "python",
      "name": "python2"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}